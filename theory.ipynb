{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0714a20d-877e-47b9-a89c-fcf430db06ca",
   "metadata": {},
   "source": [
    "# Transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ceddc-0d29-4bd9-80f2-af94da676f1f",
   "metadata": {},
   "source": [
    "Imagine you're reading a sentence. When you read each word, you naturally consider how it relates to other words around it. Transformer models work in a similar way, but they do this with mathematical relationships. Here's how:\n",
    "\n",
    "1. First Pass - Breaking Things Down:\n",
    "\n",
    "- The model takes your input (like a sentence) and breaks each word into numerical values\n",
    "- It also adds information about where each word appears in the sentence (like labels saying \"first word,\" \"second word,\" etc.)\n",
    "\n",
    "2. The Attention Mechanism (The Special Sauce):\n",
    "This is the key innovation of transformers. Think of it like having multiple spotlights that can shine on different words at once:\n",
    "\n",
    "- For each word, the model looks at ALL other words in the sentence simultaneously\n",
    "- It figures out how important each connection is (like how \"bark\" might strongly connect to \"dog\" but weakly to \"the\")\n",
    "- It can handle long-range connections that earlier models struggled with\n",
    "\n",
    "\n",
    "3. Processing Information:\n",
    "\n",
    "- The model processes all these connections in parallel, rather than one after another\n",
    "- This makes it much faster than older models that had to process words in sequence\n",
    "\n",
    "\n",
    "4. Making Sense of It All:\n",
    "\n",
    "- After looking at all these connections, the model combines this information to understand the context\n",
    "- This helps it predict what comes next or understand the meaning of ambiguous words\n",
    "\n",
    "A real-world analogy would be like being in a room full of people having a conversation. You don't just listen to one person at a time - you're aware of everyone's contributions and how they relate to each other, all at once. That's similar to how transformers process information.\n",
    "\n",
    "This architecture has revolutionized AI because it:\n",
    "\n",
    "- Processes information more efficiently than older models\n",
    "- Understands context better\n",
    "- Can handle longer pieces of text\n",
    "- Works well for many different language tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46ef74-cd58-4c00-af13-dc8d65cfa4ca",
   "metadata": {},
   "source": [
    "### Below is a Diagram of the Transformer Architecture\n",
    "<img src='./dia.png' width='500' height='700' style='margin-left:auto; margin-right:auto' /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef01751-41f8-4aef-815c-ff1ab210d7c4",
   "metadata": {},
   "source": [
    "Now, Let`s break down each component in detail: \n",
    "\n",
    "1. Input Processing Layer:\n",
    "   - Input Embeddings: Converts word/tokens into dense vectors (typically 512-1024 dimensions)\n",
    "   - Positional Encodings: Adds Information about token position using sine and cosine function\n",
    "   - These combine to give each token both meaning and positional information\n",
    "2. Encoder Block:\n",
    "   - Multi-Head Self-Attention:\n",
    "     - Calculates three main matrices for each token: Query(Q), Key(V), and Value(V)\n",
    "     - Computes attention scores between all tokens using this formula: $(Q * K^T)/\\sqrt{dk}\\ $\n",
    "     - Uses Multiple heads to capture different types of relationships\n",
    "     - Each head focuses on different aspects of the relationships b/w words\n",
    "   - Add & Normalize Layer:\n",
    "     - Residual connection that adds the input to attention output\n",
    "     - Layer normalization to stabalize the network\n",
    "   - Feed-Forward Network:\n",
    "     - Two Linear Transformations with a ReLU activation in between\n",
    "     - Processes each position independtly\n",
    "     - usually increases dimensionality then reduces it back\n",
    "3. Decoder Block:\n",
    "   - Masked Multi-Head Self-Attention:\n",
    "     - Similar to encoder-self Attention but with masking\n",
    "     - Prevents the model from looking at future tokens during training\n",
    "     - Essential for tasks like language generation\n",
    "   - Cross-Attention\n",
    "     - Connects decoder to encoder outputs\n",
    "     - queries come from decoder, keys and values come from encoder\n",
    "     - Allows decoder to focus on relevant parts of input\n",
    "   - Feed-Forward & Normaliztion\n",
    "     - Similar structure to encoder blocks\n",
    "     - Processes the combined attention information\n",
    "4. Key Supporting Features: \n",
    "   - Attention Mechanism Math:\n",
    "      $ Attention(Q,K,V) = softmax(QK^T/\\sqrt{dk}\\)V $\n",
    "     - where: Q = Query matrix, K = Key matrix, V = Value matrix, dk = dimension of keys\n",
    "   - Scaling Factors:\n",
    "     - $\\sqrt{dk}\\ $ prevents vanishing gradients in large sequences\n",
    "     - Multiple attention heads typically use dk = 64\n",
    "5. Output Layer:\n",
    "   - Linear projection to vocab size\n",
    "   - softmax function to convert to probabilites\n",
    "   - Final Predictions for the target sequence\n",
    "\n",
    "\n",
    "Special Characteristics:\n",
    "\n",
    "1. Parallelization:\n",
    "- All positions are processed simultaneously\n",
    "- Massive speedup compared to sequential models\n",
    "\n",
    "2. Information Flow:\n",
    "- Direct connections between any two positions\n",
    "- Maximum path length is O(1) between any two tokens\n",
    "- Helps with long-range dependencies\n",
    "\n",
    "3. Training Optimizations:\n",
    "- Label smoothing\n",
    "- Learning rate warmup\n",
    "- Dropout in various components\n",
    "- Layer normalization\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
