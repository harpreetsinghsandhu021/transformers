{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ae4b3e-c584-4849-82e1-22fdd3ee5b8c",
   "metadata": {},
   "source": [
    "## Fine-Tuning a Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01701324-1da7-429a-88aa-7449ca81388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer, create_optimizer\n",
    "\n",
    "from datasets import Dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c0b8d8-a5c4-4f2d-96ba-91dc21f095e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth enabled\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(\"Memory growth enabled\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error enabling memory growth: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13afadc9-117b-4bef-948b-a781372658e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = load_from_disk('./data/drug-reviews/') # This is the processed drugs dataset we processed in datasets.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f206dcd7-86b7-4d5a-a3d2-ee1fbb0731f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5727d6a6-59bd-4a09-a705-684a16e2b2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 89879,\n",
       " 'drugName': 'Cyclosporine',\n",
       " 'condition': 'keratoconjunctivitis sicca',\n",
       " 'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"',\n",
       " 'rating': 2.0,\n",
       " 'date': 'April 20, 2013',\n",
       " 'usefulCount': 69,\n",
       " 'review_length': 147}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d64bce8-5c8b-4bec-aba1-ac0e9a45b248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 19:09:44.142873: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-19 19:09:44.142975: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-19 19:09:44.142998: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-19 19:09:44.143290: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-19 19:09:44.143557: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'facebook/bart-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd154c3e-9768-4813-8471-931d70804d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summaries(example): \n",
    "    \"\"\" \n",
    "    Purpose: Create a summary combining drug name, condition, and rating\n",
    "    \"\"\"\n",
    "    example['summary'] = f\"Patient review of {example['drugName']} for {example['condition']}. Rating: {example['rating']}/5\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d024cd-5f97-44c2-a5db-f730c31f263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(examples): \n",
    "    \"\"\" \n",
    "    Purpose: Tokenize inputs and targets\n",
    "    \"\"\"\n",
    "    inputs = examples['review']\n",
    "    targets = examples['summary'] \n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=512, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer(): \n",
    "        labels = tokenizer(\n",
    "        targets, \n",
    "        max_length=128, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": model_inputs['input_ids'], \n",
    "        \"attention_mask\": model_inputs['attention_mask'], \n",
    "        \"labels\": labels['input_ids']\n",
    "    }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d16320c-986b-4971-97a6-0d154fb776c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(dataset, batch_size=16): \n",
    "    \"\"\"\n",
    "    Purpose: Convert Hugging Face dataset to TF dataset \n",
    "    \"\"\"\n",
    "    dataset = dataset.map(create_summaries) # This add summary field to the dataset \n",
    "\n",
    "    processed_dataset = dataset.map(prepare_data, remove_columns=dataset.column_names, batched=True, batch_size=100) # tokenize data for model to make sense \n",
    "\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "        'input_ids': processed_dataset['input_ids'], \n",
    "        'attention_mask': processed_dataset['attention_mask'], \n",
    "        'labels': processed_dataset['labels'], \n",
    "    })\n",
    "\n",
    "    return tf_dataset.map(\n",
    "        lambda x: (\n",
    "            {\n",
    "                \"input_ids\": x[\"input_ids\"], \n",
    "                \"attention_mask\": x[\"attention_mask\"]\n",
    "            }, \n",
    "            x['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f60f7acf-6b66-4efc-9324-85feaa17b17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f53172990045d080d7ef74ca20b00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 \n",
    "train_dataset = create_tf_dataset(drug_dataset['train'], batch_size)\n",
    "val_dataset = create_tf_dataset(drug_dataset['validation'], batch_size)\n",
    "test_dataset = create_tf_dataset(drug_dataset['test'], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "318dcd9c-6465-40ba-8266-71819c3bd745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
      "array([    0,   113,   100,    33,   341,  8426, 17048,    13,    59,\n",
      "          10,    76,   122,     8,    33,   450,   818,   117,  2017,\n",
      "           4,  1437,   286,   144,     9,   127,   301,    38,   348,\n",
      "          56,  1275,     8, 15304, 12465,  2473,     4,   572,   667,\n",
      "        1337,  2295,  9305,     6,   127,  3299,  5131,  8426, 17048,\n",
      "           4,  1437,    91,    26,    24,  3700,  1239,   155,     7,\n",
      "         231,   377,    13,    24,     7,   269,  3151,    11,    53,\n",
      "          24,   393,   222,  3151,    11,     4,  1437,   520,    38,\n",
      "         342,     5,  9305,    11,    24, 14827,   127,  2473,    13,\n",
      "           5,    78,   389,   111,   843,   728,     4,  1437,    38,\n",
      "         348,  3244,    19,   127,  3299,    59,    42,     8,    37,\n",
      "          26,    24,    16,  2340,    53,   197,   213,   409,    71,\n",
      "         103,    86,     6,    53,    24,  2282,    75,     4,  4337,\n",
      "          76,   198,  2428,    86,   127,  2473,   120,  6587, 35270,\n",
      "        1437,     8,    42,    76,    34,    57,     5,   276,    36,\n",
      "       31501,   190,  3007,    87,    97,   107,    43,   190,   600,\n",
      "          38,   348,    57,   634,  8426, 17048,    13,    10,    76,\n",
      "         122,     4,    20,   129,  2249,    38,  3120,    21,    13,\n",
      "           5,    78,   891,   688,     6,    53,   122,    38,   437,\n",
      "        1227,     7,   517,    15,    72,     2,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
      "array([    0, 18276,  4843,  1551,     9, 25544,   366, 12150,   833,\n",
      "          13, 34831,   415, 33955,   267, 27688,  1879, 10100,   579,\n",
      "         636,  3245,     4,  8998,    35,   132,     4,   288,    73,\n",
      "         245,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset.take(1)\n",
    "for example in sample:  \n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6fb7e725-c792-478b-ba68-bceb5d6628f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "\n",
    "num_train_steps = (len(drug_dataset['train']) // batch_size) * 3\n",
    "initial_learning_rate = 2e-5\n",
    "num_warmup_steps = num_train_steps // 10 # got to warmup those muscles buddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d8e8584-11bf-4eac-a6e9-dd8502180674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Learning rate schedule with warmp \n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate, \n",
    "    num_train_steps - num_warmup_steps, \n",
    "    end_learning_rate=0.0, \n",
    "    power=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "176407af-1f43-4ab7-83b0-c744606606be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=initial_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cde476d1-27ed-4612-9820-f3a4a19b2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=None, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1b2e52c-23c3-4bf7-aa53-b5de114aecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be5cd384-a791-4b71-b84c-84fc0b19d8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileo2tlt_c4.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileqkmxk_y2.py\", line 53, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).model, (ag__.ld(input_ids),), dict(attention_mask=ag__.ld(attention_mask), decoder_input_ids=ag__.ld(decoder_input_ids), encoder_outputs=ag__.ld(encoder_outputs), decoder_attention_mask=ag__.ld(decoder_attention_mask), decoder_position_ids=ag__.ld(decoder_position_ids), head_mask=ag__.ld(head_mask), decoder_head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(inputs_embeds), decoder_inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileo2tlt_c4.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py\", line 39, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(decoder_input_ids) is None, lambda : ag__.ld(decoder_inputs_embeds) is None), if_body_1, else_body_1, get_state_1, set_state_1, ('decoder_input_ids',), 1)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py\", line 34, in if_body_1\n        decoder_input_ids = ag__.converted_call(ag__.ld(shift_tokens_right), (ag__.ld(input_ids), ag__.ld(self).config.pad_token_id, ag__.ld(self).config.decoder_start_token_id), None, fscope)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filelo5bpi1q.py\", line 14, in tf__shift_tokens_right\n        shifted_input_ids = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(start_tokens), ag__.ld(input_ids)[:, :-1]], -1), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_bart_for_conditional_generation' (type TFBartForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1435, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/models/bart/modeling_tf_bart.py\", line 1465, in call  *\n            outputs = self.model(\n        File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileo2tlt_c4.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py\", line 39, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(decoder_input_ids) is None, lambda : ag__.ld(decoder_inputs_embeds) is None), if_body_1, else_body_1, get_state_1, set_state_1, ('decoder_input_ids',), 1)\n        File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py\", line 34, in if_body_1\n            decoder_input_ids = ag__.converted_call(ag__.ld(shift_tokens_right), (ag__.ld(input_ids), ag__.ld(self).config.pad_token_id, ag__.ld(self).config.decoder_start_token_id), None, fscope)\n        File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filelo5bpi1q.py\", line 14, in tf__shift_tokens_right\n            shifted_input_ids = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(start_tokens), ag__.ld(input_ids)[:, :-1]], -1), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'model' (type TFBartMainLayer).\n        \n        in user code:\n        \n            File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1435, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/models/bart/modeling_tf_bart.py\", line 1191, in call  *\n                decoder_input_ids = shift_tokens_right(\n            File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/models/bart/modeling_tf_bart.py\", line 71, in shift_tokens_right  *\n                shifted_input_ids = tf.concat([start_tokens, input_ids[:, :-1]], -1)\n        \n            ValueError: Index out of range using input dim 1; input has only 1 dims for '{{node tf_bart_for_conditional_generation/model/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=3, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=0](IteratorGetNext:1, tf_bart_for_conditional_generation/model/strided_slice/stack, tf_bart_for_conditional_generation/model/strided_slice/stack_1, tf_bart_for_conditional_generation/model/strided_slice/stack_2)' with input shapes: [512], [2], [2], [2] and with computed input tensors: input[3] = <1 1>.\n        \n        \n        Call arguments received by layer 'model' (type TFBartMainLayer):\n          • input_ids=tf.Tensor(shape=(512,), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(512,), dtype=int32)\n          • decoder_input_ids=None\n          • decoder_attention_mask=None\n          • decoder_position_ids=None\n          • head_mask=None\n          • decoder_head_mask=None\n          • cross_attn_head_mask=None\n          • encoder_outputs=None\n          • past_key_values=None\n          • inputs_embeds=None\n          • decoder_inputs_embeds=None\n          • use_cache=True\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n          • kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer 'tf_bart_for_conditional_generation' (type TFBartForConditionalGeneration):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(512,), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(512,), dtype=int32)'}\n      • attention_mask=None\n      • decoder_input_ids=None\n      • decoder_attention_mask=None\n      • decoder_position_ids=None\n      • head_mask=None\n      • decoder_head_mask=None\n      • cross_attn_head_mask=None\n      • encoder_outputs=None\n      • past_key_values=None\n      • inputs_embeds=None\n      • decoder_inputs_embeds=None\n      • use_cache=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filekywxnlay.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1672\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1670\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1672\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss:\n\u001b[1;32m   1674\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_loss(y_pred\u001b[38;5;241m.\u001b[39mloss, y_pred\u001b[38;5;241m.\u001b[39mloss, sample_weight, regularization_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n",
      "File \u001b[0;32m/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileo2tlt_c4.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileqkmxk_y2.py:53\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, decoder_position_ids, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     52\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(labels) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel, (ag__\u001b[38;5;241m.\u001b[39mld(input_ids),), \u001b[38;5;28mdict\u001b[39m(attention_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(attention_mask), decoder_input_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(decoder_input_ids), encoder_outputs\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(encoder_outputs), decoder_attention_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(decoder_attention_mask), decoder_position_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(decoder_position_ids), head_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(head_mask), decoder_head_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(decoder_head_mask), cross_attn_head_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(cross_attn_head_mask), past_key_values\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(past_key_values), inputs_embeds\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs_embeds), decoder_inputs_embeds\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(decoder_inputs_embeds), use_cache\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(use_cache), output_attentions\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_attentions), output_hidden_states\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_hidden_states), return_dict\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(return_dict), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     54\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmatmul, (ag__\u001b[38;5;241m.\u001b[39mld(outputs)[\u001b[38;5;241m0\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mshared\u001b[38;5;241m.\u001b[39mweights), \u001b[38;5;28mdict\u001b[39m(transpose_b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[1;32m     55\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mbias_layer, (ag__\u001b[38;5;241m.\u001b[39mld(lm_logits),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileo2tlt_c4.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py:39\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, decoder_position_ids, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m decoder_input_ids\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mand_(\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(decoder_input_ids) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(decoder_inputs_embeds) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_4\u001b[39m():\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (encoder_outputs,)\n",
      "File \u001b[0;32m/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     33\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(input_ids) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body, else_body, get_state, set_state, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m decoder_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_tokens_right\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filelo5bpi1q.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__shift_tokens_right\u001b[0;34m(input_ids, pad_token_id, decoder_start_token_id)\u001b[0m\n\u001b[1;32m     12\u001b[0m decoder_start_token_id \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(decoder_start_token_id), ag__\u001b[38;5;241m.\u001b[39mld(input_ids)\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m start_tokens \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfill, ((ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(shape_list), (ag__\u001b[38;5;241m.\u001b[39mld(input_ids),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconvert_to_tensor, (ag__\u001b[38;5;241m.\u001b[39mld(decoder_start_token_id), ag__\u001b[38;5;241m.\u001b[39mld(input_ids)\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 14\u001b[0m shifted_input_ids \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconcat, ([ag__\u001b[38;5;241m.\u001b[39mld(start_tokens), ag__\u001b[38;5;241m.\u001b[39mld(input_ids)[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m shifted_input_ids \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mwhere, (ag__\u001b[38;5;241m.\u001b[39mld(shifted_input_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfill, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(shape_list), (ag__\u001b[38;5;241m.\u001b[39mld(shifted_input_ids),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconvert_to_tensor, (ag__\u001b[38;5;241m.\u001b[39mld(pad_token_id), ag__\u001b[38;5;241m.\u001b[39mld(input_ids)\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__\u001b[38;5;241m.\u001b[39mld(shifted_input_ids)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m assert_gte0 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mdebugging\u001b[38;5;241m.\u001b[39massert_greater_equal, (ag__\u001b[38;5;241m.\u001b[39mld(shifted_input_ids), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconstant, (\u001b[38;5;241m0\u001b[39m,), \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(input_ids)\u001b[38;5;241m.\u001b[39mdtype), fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileo2tlt_c4.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileqkmxk_y2.py\", line 53, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).model, (ag__.ld(input_ids),), dict(attention_mask=ag__.ld(attention_mask), decoder_input_ids=ag__.ld(decoder_input_ids), encoder_outputs=ag__.ld(encoder_outputs), decoder_attention_mask=ag__.ld(decoder_attention_mask), decoder_position_ids=ag__.ld(decoder_position_ids), head_mask=ag__.ld(head_mask), decoder_head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(inputs_embeds), decoder_inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileo2tlt_c4.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py\", line 39, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(decoder_input_ids) is None, lambda : ag__.ld(decoder_inputs_embeds) is None), if_body_1, else_body_1, get_state_1, set_state_1, ('decoder_input_ids',), 1)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py\", line 34, in if_body_1\n        decoder_input_ids = ag__.converted_call(ag__.ld(shift_tokens_right), (ag__.ld(input_ids), ag__.ld(self).config.pad_token_id, ag__.ld(self).config.decoder_start_token_id), None, fscope)\n    File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filelo5bpi1q.py\", line 14, in tf__shift_tokens_right\n        shifted_input_ids = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(start_tokens), ag__.ld(input_ids)[:, :-1]], -1), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_bart_for_conditional_generation' (type TFBartForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1435, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/models/bart/modeling_tf_bart.py\", line 1465, in call  *\n            outputs = self.model(\n        File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_fileo2tlt_c4.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py\", line 39, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(decoder_input_ids) is None, lambda : ag__.ld(decoder_inputs_embeds) is None), if_body_1, else_body_1, get_state_1, set_state_1, ('decoder_input_ids',), 1)\n        File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filex6z9qu7j.py\", line 34, in if_body_1\n            decoder_input_ids = ag__.converted_call(ag__.ld(shift_tokens_right), (ag__.ld(input_ids), ag__.ld(self).config.pad_token_id, ag__.ld(self).config.decoder_start_token_id), None, fscope)\n        File \"/var/folders/qj/_xr448610yg3dn_k91tss4s80000gn/T/__autograph_generated_filelo5bpi1q.py\", line 14, in tf__shift_tokens_right\n            shifted_input_ids = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(start_tokens), ag__.ld(input_ids)[:, :-1]], -1), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'model' (type TFBartMainLayer).\n        \n        in user code:\n        \n            File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1435, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/models/bart/modeling_tf_bart.py\", line 1191, in call  *\n                decoder_input_ids = shift_tokens_right(\n            File \"/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/models/bart/modeling_tf_bart.py\", line 71, in shift_tokens_right  *\n                shifted_input_ids = tf.concat([start_tokens, input_ids[:, :-1]], -1)\n        \n            ValueError: Index out of range using input dim 1; input has only 1 dims for '{{node tf_bart_for_conditional_generation/model/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=3, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=0](IteratorGetNext:1, tf_bart_for_conditional_generation/model/strided_slice/stack, tf_bart_for_conditional_generation/model/strided_slice/stack_1, tf_bart_for_conditional_generation/model/strided_slice/stack_2)' with input shapes: [512], [2], [2], [2] and with computed input tensors: input[3] = <1 1>.\n        \n        \n        Call arguments received by layer 'model' (type TFBartMainLayer):\n          • input_ids=tf.Tensor(shape=(512,), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(512,), dtype=int32)\n          • decoder_input_ids=None\n          • decoder_attention_mask=None\n          • decoder_position_ids=None\n          • head_mask=None\n          • decoder_head_mask=None\n          • cross_attn_head_mask=None\n          • encoder_outputs=None\n          • past_key_values=None\n          • inputs_embeds=None\n          • decoder_inputs_embeds=None\n          • use_cache=True\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n          • kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer 'tf_bart_for_conditional_generation' (type TFBartForConditionalGeneration):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(512,), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(512,), dtype=int32)'}\n      • attention_mask=None\n      • decoder_input_ids=None\n      • decoder_attention_mask=None\n      • decoder_position_ids=None\n      • head_mask=None\n      • decoder_head_mask=None\n      • cross_attn_head_mask=None\n      • encoder_outputs=None\n      • past_key_values=None\n      • inputs_embeds=None\n      • decoder_inputs_embeds=None\n      • use_cache=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset, \n",
    "    epochs=3,\n",
    "    callbacks=[\n",
    "        tensorboard_callback,\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
