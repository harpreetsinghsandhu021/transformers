{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b5813f-b1e9-493d-842b-46c2a054c30a",
   "metadata": {},
   "source": [
    "# Semantic Search with FAISS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4857eb3d-ef27-418c-8ba4-c6776be415ec",
   "metadata": {},
   "source": [
    "In this notebook, we will build a search engine that can help us find answers to questions related to a datset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c97943-0701-4aaa-8e49-8e1a447e6611",
   "metadata": {},
   "source": [
    "## Using Embeddings for Semantic Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c1ce6-207b-40a9-8d5a-fd956bf458de",
   "metadata": {},
   "source": [
    "As we know, Transformer based language models represent each token in a span of text as an embedding vector. It turns out that one can pool the individual embeddings to create a vector representation of whole sentences, paras or even documents. These embeddings can then be used to find similar documents in the corpus by computing the dot-product similarity (or some other similarity metric) between each embedding and returning the documents with the greatest overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84444409-5fd9-4447-889d-413426564976",
   "metadata": {},
   "source": [
    "In this notebook we‚Äôll use embeddings to develop a semantic search engine. These search engines offer several advantages over conventional approaches that are based on matching keywords in a query with the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a94f0c-6369-49dc-9716-4daf18e7cd17",
   "metadata": {},
   "source": [
    "## Loading and Preparing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88f3ca0-0090-4ce5-802b-d6ddf5415c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a1d1303-d1cb-417a-ba8d-cba4645bee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = load_dataset('lewtun/github-issues', split='train')\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9fcf1-6540-4bf5-9f2c-ac15f79e482a",
   "metadata": {},
   "source": [
    "Here we‚Äôve specified the default train split in load_dataset(), so it returns a Dataset instead of a DatasetDict. The first order of business is to filter out the pull requests, as these tend to be rarely used for answering user queries and will introduce noise in our search engine. As should be familiar by now, we can use the Dataset.filter() function to exclude these rows in our dataset. While we‚Äôre at it, let‚Äôs also filter out rows with no comments, since these provide no answers to user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace5904c-28c5-412f-9fc1-afcd188d971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset = issues_dataset.filter(lambda x: x['is_pull_request'] == False and len(x['comments']) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f99190-4a74-4808-8325-c8991b137cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98882f2-fa1f-4d75-9820-211a5c9a81c7",
   "metadata": {},
   "source": [
    "Well, We're done to 808 rows from 3019.In short, Don`t let the numbers scare you .We can see that there are a lot of columns in our dataset, most of which we don‚Äôt need to build our search engine. From a search perspective, the most informative columns are title, body, and comments, while html_url provides us with a link back to the source issue. Let‚Äôs use the Dataset.remove_columns() function to drop the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35141b6-f27a-4acf-8ee6-1d47f5a86378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_include = ['title','body','comments','html_url']\n",
    "colums_to_exclue = set(columns_to_include).symmetric_difference(columns)\n",
    "\n",
    "issues_dataset = issues_dataset.remove_columns(colums_to_exclue)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e9a3928-9a2e-4015-be03-adaa8616250b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset[0]['comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b32619-cb30-4dad-90bf-190811c81ecc",
   "metadata": {},
   "source": [
    "To create our embeddings we‚Äôll augment each comment with the issue‚Äôs title and body, since these fields often include useful contextual information. Because our comments column is currently a list of comments for each issue, we need to ‚Äúexplode‚Äù the column so that each row consists of an (html_url, title, body, comment) tuple. In Pandas we can do this with the DataFrame.explode() function, which creates a new row for each element in a list-like column, while replicating all the other column values. To see this in action, let‚Äôs first switch to the Pandas DataFrame format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd990b60-9790-4c78-b921-d0773535f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset.set_format('pandas')\n",
    "dataframe = issues_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07e4ad7-7bc6-42c5-9f7f-460b269bd9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should hav...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplit...</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is al...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>load_dataset using default cache on Windows ca...</td>\n",
       "      <td>[Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...</td>\n",
       "      <td>## Describe the bug\\r\\nStandard process to dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>to_tf_dataset keeps a reference to the open da...</td>\n",
       "      <td>[I did some investigation and, as it seems, th...</td>\n",
       "      <td>To reproduce:\\r\\n```python\\r\\nimport datasets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Conda build fails</td>\n",
       "      <td>[Why 1.9 ?\\r\\n\\r\\nhttps://anaconda.org/Hugging...</td>\n",
       "      <td>## Describe the bug\\r\\nCurrent `datasets` vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Mutable columns argument breaks set_format</td>\n",
       "      <td>[Pushed a fix to my branch #2731 ]</td>\n",
       "      <td>## Describe the bug\\r\\nIf you pass a mutable l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Datasets 1.12 dataset.filter TypeError: get_in...</td>\n",
       "      <td>[Thanks for reporting, I'm looking into it :),...</td>\n",
       "      <td>## Describe the bug\\r\\nUpgrading to 1.12 cause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>\"File name too long\" error for file locks</td>\n",
       "      <td>[Hi, the filename here is less than 255\\r\\n```...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nGetting the followi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Unwanted progress bars when accessing examples</td>\n",
       "      <td>[doing a patch release now :)]</td>\n",
       "      <td>When accessing examples from a dataset formatt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "5  https://github.com/huggingface/datasets/issues...   \n",
       "6  https://github.com/huggingface/datasets/issues...   \n",
       "7  https://github.com/huggingface/datasets/issues...   \n",
       "8  https://github.com/huggingface/datasets/issues...   \n",
       "9  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1  Backwards compatibility broken for cached data...   \n",
       "2  OSCAR unshuffled_original_ko: NonMatchingSplit...   \n",
       "3  load_dataset using default cache on Windows ca...   \n",
       "4  to_tf_dataset keeps a reference to the open da...   \n",
       "5                                  Conda build fails   \n",
       "6         Mutable columns argument breaks set_format   \n",
       "7  Datasets 1.12 dataset.filter TypeError: get_in...   \n",
       "8          \"File name too long\" error for file locks   \n",
       "9     Unwanted progress bars when accessing examples   \n",
       "\n",
       "                                            comments  \\\n",
       "0  [Cool, I think we can do both :), @lhoestq now...   \n",
       "1  [Hi ! I guess the caching mechanism should hav...   \n",
       "2  [I tried `unshuffled_original_da` and it is al...   \n",
       "3  [Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...   \n",
       "4  [I did some investigation and, as it seems, th...   \n",
       "5  [Why 1.9 ?\\r\\n\\r\\nhttps://anaconda.org/Hugging...   \n",
       "6                 [Pushed a fix to my branch #2731 ]   \n",
       "7  [Thanks for reporting, I'm looking into it :),...   \n",
       "8  [Hi, the filename here is less than 255\\r\\n```...   \n",
       "9                     [doing a patch release now :)]   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "2  ## Describe the bug\\r\\n\\r\\nCannot download OSC...  \n",
       "3  ## Describe the bug\\r\\nStandard process to dow...  \n",
       "4  To reproduce:\\r\\n```python\\r\\nimport datasets ...  \n",
       "5  ## Describe the bug\\r\\nCurrent `datasets` vers...  \n",
       "6  ## Describe the bug\\r\\nIf you pass a mutable l...  \n",
       "7  ## Describe the bug\\r\\nUpgrading to 1.12 cause...  \n",
       "8  ## Describe the bug\\r\\n\\r\\nGetting the followi...  \n",
       "9  When accessing examples from a dataset formatt...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a7bb4-6a6d-4c52-920d-ad49941ac0fd",
   "metadata": {},
   "source": [
    "When we Explode the dataframe, We Expect to get one row for each of the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2b9c3e-2af5-4007-ac93-d22b0778ac28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Well it can cause issue with anyone that updat...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>I just merged a fix, let me know if you're sti...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Definitely works on several manual cases with ...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Fixed by #2947.</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplit...</td>\n",
       "      <td>I tried `unshuffled_original_da` and it is als...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>load_dataset using default cache on Windows ca...</td>\n",
       "      <td>Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfor...</td>\n",
       "      <td>## Describe the bug\\r\\nStandard process to dow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "5  https://github.com/huggingface/datasets/issues...   \n",
       "6  https://github.com/huggingface/datasets/issues...   \n",
       "7  https://github.com/huggingface/datasets/issues...   \n",
       "8  https://github.com/huggingface/datasets/issues...   \n",
       "9  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "4  Backwards compatibility broken for cached data...   \n",
       "5  Backwards compatibility broken for cached data...   \n",
       "6  Backwards compatibility broken for cached data...   \n",
       "7  Backwards compatibility broken for cached data...   \n",
       "8  OSCAR unshuffled_original_ko: NonMatchingSplit...   \n",
       "9  load_dataset using default cache on Windows ca...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "4  Well it can cause issue with anyone that updat...   \n",
       "5  I just merged a fix, let me know if you're sti...   \n",
       "6  Definitely works on several manual cases with ...   \n",
       "7                                    Fixed by #2947.   \n",
       "8  I tried `unshuffled_original_da` and it is als...   \n",
       "9  Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfor...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "4  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "5  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "6  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "7  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "8  ## Describe the bug\\r\\n\\r\\nCannot download OSC...  \n",
       "9  ## Describe the bug\\r\\nStandard process to dow...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataframe = dataframe.explode('comments', ignore_index=True)\n",
    "comments_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a2358-6e21-4474-b20b-3fef1f3c486f",
   "metadata": {},
   "source": [
    "Good job, we can see that the rows have been replicated with comments column containing individual comments, Now Switch back to to dataset from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef734add-5006-4bd3-9436-3a33b851a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dcdb27d-3259-48a0-b0dc-910124826bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = Dataset.from_pandas(comments_dataframe)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb9fdc-855c-47bc-9d77-fcf7111e200b",
   "metadata": {},
   "source": [
    "well, turns out the number of rows has substantially increased from 808 to 2964"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3775d-6811-4d69-b517-78643f483bf1",
   "metadata": {},
   "source": [
    "Now, that we have one comment per row, let`s create a new comments_length column that contains the number of words per comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff8129e2-13a4-4fd6-9419-6cd44244e0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f8a5713e784050b5e49d20fd1f1bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(lambda x: {\"comment_length\": len(x['comments'].split())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c517c3df-04f9-4464-91b7-e1c1d7da23de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/2945',\n",
       " 'title': 'Protect master branch',\n",
       " 'comments': 'Cool, I think we can do both :)',\n",
       " 'body': 'After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       " 'comment_length': 8}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f9fed-8686-4e2c-ad42-bccc5f7f528c",
   "metadata": {},
   "source": [
    "We can use this new column to filter out short comments, which typically include things like ‚Äúcc @lewtun‚Äù or ‚ÄúThanks!‚Äù that are not relevant for our search engine. There‚Äôs no precise number to select for the filter, but around 15 words seems like a good start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "875dc84c-61c5-4d76-80a3-dfc6b6607eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489be8f276084e98ab1d598f8fdb23b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x['comment_length'] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51453d9-c2a7-45df-8941-5e3e7dfc0b6f",
   "metadata": {},
   "source": [
    "well, turns out the number of rows has descreased from 2964 to 2175"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45dcd8-9ac7-4c85-9fc4-3441bba6b5c5",
   "metadata": {},
   "source": [
    "let‚Äôs concatenate the issue title, body, and comments together in a new text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "999fb847-ffc2-4410-9306-51163968d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e137e6f7-8305-42e6-9a53-37cdac03812f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317a53a460e548f8befbb1bcb6bf85c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(concatenate_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5818f6c-c86d-42b3-b8fe-df5150f0d7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9519efb-2235-4f0b-88f2-ab8630763ee4",
   "metadata": {},
   "source": [
    "## Creating Text Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b60361-b574-4ac4-8ebc-fe90d35db4a6",
   "metadata": {},
   "source": [
    "we know that we can obtain token embeddings by using the AutoModel class. All we need to do us pick a suitable checkpoint to load the model from. Fortunately, there‚Äôs a library called `sentence-transformers` that is dedicated to creating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caa34527-8cf2-4207-a797-678bde64ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51ec53cb-b84c-416f-b9fa-6ab1c250bd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 12:30:28.794615: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-21 12:30:28.794636: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-21 12:30:28.794642: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-21 12:30:28.794671: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-21 12:30:28.794982: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMPNetModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = TFAutoModel.from_pretrained(model_checkpoint,from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9033e-dd5f-4024-bee1-1ae25ac47f35",
   "metadata": {},
   "source": [
    "We`d like to represent each entry in our github issues corpus as a single vector, so we need to pool or average our token embeddings in some way. One of the popular approach is to perform *CLS Pooling* on our model's outputs, where we simply collect the last hidden state for the special [CLS] token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8df35857-dc91-4da5-9a71-a6e0fb7ddc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output): \n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb579e-972e-4ced-8a76-c0c66947eb4c",
   "metadata": {},
   "source": [
    "Next, we create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba13531b-9c91-488c-b31d-0a65824a50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list): \n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors='tf') # tokenize the text\n",
    "    encoded_input = {key:value for key, value in encoded_input.items()} \n",
    "    model_output = model(**encoded_input) # feed tokenized inputs to the model \n",
    "\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b22690b-8946-4d2d-a23e-fe5ab0eb8773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Protect master branch \\n After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution. \\n @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd05889a-13db-41de-8690-7ff37b47f2d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the above function by feeding it the first text entry in our corpus and inspecting the output shape \n",
    "embedding = get_embeddings(comments_dataset['text'][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64dce82-5228-4078-86ec-20ef7191067d",
   "metadata": {},
   "source": [
    "Great, we‚Äôve converted the first entry in our corpus into a 768-dimensional vector! We can use Dataset.map() to apply our get_embeddings() function to each row in our corpus, so let‚Äôs create a new embeddings column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffc3a4-539c-49d4-8cea-722dbde3b673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94270c0fbd34f6fbe3a305678d3f2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map( lambda x: {\"embeddings\" : get_embeddings(x[\"text\"]).numpy()[0] } )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9a951-ac96-44d7-8d2c-1478dfd1ebed",
   "metadata": {},
   "source": [
    "Notice that we‚Äôve converted the embeddings to NumPy arrays ‚Äî that‚Äôs because ü§ó Datasets requires this format when we try to index them with FAISS, which we`ll do next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de943549-e0b5-466d-8dc2-a30ae734c948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fe8d6-5b3d-4d84-b5f3-2899c0277ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
