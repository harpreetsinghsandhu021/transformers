{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d7c0a-85ed-420d-9846-f555af71dbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79e522a-2c5e-4e49-ae86-a1451e300a16",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Pre-Trained Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25215179-39a0-4848-b49d-b2e7d2d8fe5a",
   "metadata": {},
   "source": [
    "GOALS: \n",
    "- How to prepare a large dataset from the Hub\n",
    "- How to use Keras to fine-tune a model\n",
    "- How to use Keras to get predictions\n",
    "- How to use a custom metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587b85e-d552-45c4-87f2-cff769170e59",
   "metadata": {},
   "source": [
    "## Processing the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef39ae-bd3c-4b7a-bfe3-33e28b3eb851",
   "metadata": {},
   "source": [
    "Here is how to train a sequence classifier on one batch in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af840cb-65d6-4593-9ee2-e2ed3c1ed386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5ff6ea-c205-43b1-95ab-fddc63417802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth enabled\n"
     ]
    }
   ],
   "source": [
    "# Must be done at the very start, before any other TensorFlow operations\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(\"Memory growth enabled\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error enabling memory growth: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa794d4-e599-45cd-9f96-0583338f144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 13:08:38.205518: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-19 13:08:38.205542: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-19 13:08:38.205547: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-19 13:08:38.205581: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-19 13:08:38.205787: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce90a71-903e-4946-82f2-d029dbc2bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8803689-06ba-45ce-a4a4-f5aeae522c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
       " array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
       "         12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], dtype=int32)>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
       " 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors='tf'))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f042fbb8-7ec9-4231-a699-1d7892efc0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24a5f76-0257-4729-a568-0fbc54138e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 1], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tf.convert_to_tensor([1, 1])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258da3d9-21ec-41cc-b748-4dec440ebfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 13:08:44.256483: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.49500846862793"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_on_batch(batch, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158cacc-22ca-4345-b7d6-f0bfb695f24f",
   "metadata": {},
   "source": [
    "In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a paper by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). We’ve selected it for this chapter because it’s a small dataset, so it’s easy to experiment with training on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc0d867-948c-4a1b-8d4d-e8a61a014534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b28bc6b-4834-4355-a46b-4641e51e5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets\n",
    "raw_datasets = load_dataset('glue','mrpc')\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6622b9f9-ff34-46e4-be52-378448687809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
       " 'sentence2': \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\",\n",
       " 'label': 0,\n",
       " 'idx': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2014baf-7d77-4bb3-a51c-34d4cb5214f3",
   "metadata": {},
   "source": [
    "We can see the labels are already integers, so we won’t have to do any preprocessing there. To know which integer corresponds to which label, we can inspect the features of our raw_train_dataset. This will tell us the type of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0731938e-d685-4592-81f4-e1b8b6e4a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e4641-aad5-45ca-8206-bd7eef062df9",
   "metadata": {},
   "source": [
    "Behind the scenes, label is of type ClassLabel, and the mapping of integers to label name is stored in the names folder. 0 corresponds to `not_equivalent`, and 1 corresponds to `equivalent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb3a934-6be1-423a-8624-e46bae1133e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Unable to find a home for him , a judge told mental health authorities they needed to find supervised housing and treatment for DeVries somewhere in California .', 'sentence2': 'The judge had told the state Department of Mental Health to find supervised housing and treatment for DeVries somewhere in California .', 'label': 1, 'idx': 127} \n",
      "\n",
      " {'sentence1': 'However , EPA officials would not confirm the 20 percent figure .', 'sentence2': 'Only in the past few weeks have officials settled on the 20 percent figure .', 'label': 0, 'idx': 812}\n"
     ]
    }
   ],
   "source": [
    "raw_val_dataset = raw_datasets['validation']\n",
    "print(raw_val_dataset[15], \"\\n\"\"\\n\" , raw_val_dataset[87])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8415674a-2427-46f6-93f4-7ee922dc5cfd",
   "metadata": {},
   "source": [
    "### Preprocessing a Dataset\n",
    "To preprocess the dataset, we need to convert the text to numbers the model can make sense of.We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb042718-0def-4f86-be86-5a2f341f9af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_1 = tokenizer(raw_datasets['train']['sentence1'])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets['train']['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b8a0c1-f497-44ad-8490-415f38c9a148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
       " 'They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',\n",
       " 'Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n",
       " 'The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .',\n",
       " 'Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier .',\n",
       " 'The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .',\n",
       " 'The DVD-CCA then appealed to the state Supreme Court .',\n",
       " 'That compared with $ 35.18 million , or 24 cents per share , in the year-ago period .',\n",
       " 'Shares of Genentech , a much larger company with several products on the market , rose more than 2 percent .']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['sentence1'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093109d8-5cb3-445e-a5af-87b9788c1658",
   "metadata": {},
   "source": [
    "However, we can’t just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. We need to handle the two sequences as a pair, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bac80c7-bc59-45e0-b1d3-7c929ac8a390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1dcf18-3b60-48f4-b13f-0b6566e15a62",
   "metadata": {},
   "source": [
    "`token_type_ids` is what tells the model which part of the input is the first sentence and which is the second sentence.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818d157-9358-48b0-8390-033cc580cbc6",
   "metadata": {},
   "source": [
    "Now we Take element 15 of the training set and tokenize the two sentences separately and as a pair. What’s the difference between the two results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b349806a-336d-4d17-b65c-ce44d5f16f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_1 = tokenizer(raw_train_dataset[15][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_train_dataset[15][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf5067d5-15d6-4378-9ad6-0b6c35ea23da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 24049, 2001, 2087, 3728, 3026, 3580, 2343, 2005, 1996, 9722, 1004, 4132, 9340, 12439, 2964, 2449, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e32b291-18b3-43be-b028-6108b6b1f763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3026, 3580, 2343, 4388, 24049, 1010, 3839, 2132, 1997, 1996, 9722, 1998, 4132, 9340, 12439, 2964, 3131, 1010, 2097, 2599, 1996, 2047, 9178, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58307fe6-ba0e-4e04-9be7-06fc4e42f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_pair = tokenizer(raw_train_dataset[15][\"sentence1\"], raw_train_dataset[15][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f4145a8-1f44-415c-83fe-228b9d121c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 24049, 2001, 2087, 3728, 3026, 3580, 2343, 2005, 1996, 9722, 1004, 4132, 9340, 12439, 2964, 2449, 1012, 102, 3026, 3580, 2343, 4388, 24049, 1010, 3839, 2132, 1997, 1996, 9722, 1998, 4132, 9340, 12439, 2964, 3131, 1010, 2097, 2599, 1996, 2047, 9178, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e91249-8af1-4388-a74b-6835a03f58d7",
   "metadata": {},
   "source": [
    "If we decode the IDs inside input_ids back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ac4c46f-7308-4487-b38b-e9f72c329159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs['input_ids']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90823c92-ab14-40e6-b81c-fae91327583b",
   "metadata": {},
   "source": [
    "So we see the model expects the inputs to be of the form [CLS] sentence1 [SEP] sentence2 [SEP] when there are two sentences. Aligning this with the token_type_ids gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97310fa1-b1c6-43e1-b182-a42368bbf940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n",
    "[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ccf11-ced0-4a28-8f09-8232a496dc44",
   "metadata": {},
   "source": [
    "As you can see, the parts of the input corresponding to [CLS] sentence1 [SEP] all have a token type ID of 0, while the other parts, corresponding to sentence2 [SEP], all have a token type ID of 1.\n",
    "\n",
    "Note that if you select a different checkpoint, you won’t necessarily have the token_type_ids in your tokenized inputs (for instance, they’re not returned if you use a DistilBERT model). They are only returned when the model will know what to do with them, because it has seen them during its pretraining.\n",
    "\n",
    "we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The map() method works by applying a function on each element of the dataset, so let’s define a function that tokenizes our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39508d36-ada6-40bb-baf6-9432ad829f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_element(example): \n",
    "    return tokenizer(example['sentence1'], example['sentence2'], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2f7c6-f73f-440b-996a-c8e519ddd661",
   "metadata": {},
   "source": [
    "*Note* that we’ve left the padding argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: it’s better to pad the samples when we’re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!\n",
    "\n",
    "Here is how we apply the tokenization function on all our datasets at once. We’re using batched=True in our call to map so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88bb80f6-fc50-4fb8-a952-81dedde3e166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_element, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c274003-f3d3-440c-a9ea-1a1ce5647424",
   "metadata": {},
   "source": [
    "The way the 🤗 Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function. \n",
    "\n",
    "The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together — a technique we refer to as dynamic padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23853acd-ecbb-4480-be11-72826f329641",
   "metadata": {},
   "source": [
    "## Dynamic Padding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d8400-2fee-4200-bf64-817788ca0d30",
   "metadata": {},
   "source": [
    "A collate function combines individual samples into batches. While the default collator simply converts samples to tensors and concatenates them, this won't work when samples have different sizes. Instead of padding all samples to the same length upfront (which would be inefficient), padding can be applied only as needed within each batch.\n",
    "\n",
    "\n",
    "The Transformers library provides a `DataCollatorWithPadding` class that handles this dynamic padding. When initialized with a tokenizer, it automatically adds the appropriate padding to batch items while respecting the model's padding requirements (token value and position). This approach improves training efficiency, though it may not be optimal for TPU training since TPUs perform better with fixed shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ab0e451-8758-428b-a465-50891cb4f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7590538b-0a9b-473f-a5dc-95ea275e94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78e6d4-9b6f-4031-886b-fb89c0215c3e",
   "metadata": {},
   "source": [
    "let’s grab a few samples from our training set that we would like to batch together. Here, we remove the columns idx, sentence1, and sentence2 as they won’t be needed and contain strings (and we can’t create tensors with strings) and have a look at the lengths of each entry in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3b2889d-3ecc-4f18-ac74-51d577631d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets['train'][:8]\n",
    "samples = {key:value for key, value in samples.items() if key not in ['idx', 'sentence1', 'sentence2']}\n",
    "len(samples['input_ids'][0])\n",
    "[len(x) for x in samples['input_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e64b2-3544-4476-b6d6-d110d2595346",
   "metadata": {},
   "source": [
    "No surprise, we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Let’s double-check that our data_collator is dynamically padding the batch properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36a020f6-6d8b-4831-9555-e108b4ac421d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': TensorShape([8, 67]),\n",
       " 'token_type_ids': TensorShape([8, 67]),\n",
       " 'attention_mask': TensorShape([8, 67]),\n",
       " 'labels': TensorShape([8])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k:v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d018cb-9b4a-4ee2-b0d9-367ca891799c",
   "metadata": {},
   "source": [
    "Now that we have our dataset and a data collator, we need to put them together. We could manually load batches and collate them, but that’s a lot of work, and probably not very performant either. Instead, there’s a simple method that offers a performant solution to this problem: to_tf_dataset(). This will wrap a tf.data.Dataset around your dataset, with an optional collation function. tf.data.Dataset is a native TensorFlow format that Keras can use for model.fit(), so this one method immediately converts a 🤗 Dataset to a format that’s ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35ef204f-4842-41b0-b7ae-2aed1823feaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/transformers/lib/python3.9/site-packages/datasets/arrow_dataset.py:403: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=['attention_mask', 'input_ids', 'token_type_ids'], \n",
    "    label_cols=['labels'], \n",
    "    shuffle=True, \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=['attention_mask', 'input_ids', 'token_type_ids'], \n",
    "    label_cols=['labels'], \n",
    "    shuffle=False, \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab4750-55e7-4989-b49f-c1fc07cdef39",
   "metadata": {},
   "source": [
    "## Fine-Tuning a Model with Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc27cae-aca8-43b8-a3f3-db3b2e70934d",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e1049a9-a957-42d2-adc9-ae8476eec6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075e3e7-6f1b-4d2d-a051-d40af2641945",
   "metadata": {},
   "source": [
    "We get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been inserted instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44c60e-bfc4-4510-a5e8-90c7c9e3244a",
   "metadata": {},
   "source": [
    "To fine-tune the model on our dataset, we just have to compile() our model and then pass our data to the fit() method. This will start the fine-tuning process (which should take a couple of minutes on a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0298c82d-2e64-4bef-827f-87fbf06f54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3afb9d6-414d-431c-8c12-8775a4c20e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a7531f8-9c31-4572-adb4-960aece108a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 5340s 12s/step - loss: 0.6825 - accuracy: 0.6412 - val_loss: 0.6940 - val_accuracy: 0.3162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x179476850>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441f521-0026-42df-8004-f7e488da1b66",
   "metadata": {},
   "source": [
    "We can slowly reduce the learning rate over the course of training. In the literature, you will sometimes see this referred to as decaying or annealing the learning rate. In Keras, the best way to do this is to use a learning rate scheduler. A good one to use is PolynomialDecay — despite the name, with default settings it simply linearly decays the learning rate from the initial value to the final value over the course of training, which is exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4162bd8c-5513-4ef8-9ab7-aad12f215fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54039b-d7a4-49b4-a6eb-1a45fb9074dc",
   "metadata": {},
   "source": [
    "Now we have our all-new optimizer, and we can try training with it. First, let’s reload the model, to reset the changes to the weights from the training run we just did, and then we can compile it with the new optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce9d8df-3aa7-42ec-abfb-e025fa007015",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFAutoModelForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TFAutoModelForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e32694d-79cb-40c6-89ab-ec9765379a61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(tf_train_dataset, validation_data\u001b[38;5;241m=\u001b[39mtf_validation_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
