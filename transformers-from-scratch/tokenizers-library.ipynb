{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b07f45-ab1e-48ca-9af5-0b68924a01b2",
   "metadata": {},
   "source": [
    "# Tokenizers Library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb96ee-f35b-4a58-9426-211172b4d768",
   "metadata": {},
   "source": [
    "In the Previous Notebooks, we looked at how to fine-tune a model on a given task. When we do that, we use the same tokenizer that the model was pretrained with â€” but what do we do when we want to train a model from scratch? In these cases, using a tokenizer that was pretrained on a corpus from another domain or language is typically suboptimal. For example, a tokenizer thatâ€™s trained on an English corpus will perform poorly on a corpus of Japanese texts because the use of spaces and punctuation is very different in the two languages.\n",
    "\n",
    "In this chapter, you will learn how to train a brand new tokenizer on a corpus of texts, so it can then be used to pretrain a language model. This will all be done with the help of the ğŸ¤— Tokenizers library, which provides the â€œfastâ€ tokenizers in the ğŸ¤— Transformers library. Weâ€™ll take a close look at the features that this library provides, and explore how the fast tokenizers differ from the â€œslowâ€ versions.\n",
    "\n",
    "GOALS - \n",
    "- How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts\n",
    "- The special features of fast tokenizers\n",
    "- The differences between the three main subword tokenization algorithms used in NLP today\n",
    "- How to build a tokenizer from scratch with the ğŸ¤— Tokenizers library and train it on some data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a621ab32-91ba-43f0-89e3-3886cb46cfd7",
   "metadata": {},
   "source": [
    "## Training a New Tokenizer from an Old One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17e965-b7c8-4a30-9d5a-faaa7d04e131",
   "metadata": {},
   "source": [
    " we saw that most Transformer models use a subword tokenization algorithm. To identify which subwords are of interest and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus â€” a process we call training.The exact rules that govern this training depend on the type of tokenizer used, and weâ€™ll go over the three main algorithms later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf5dc69-e59c-4218-bcb6-490dfc40d571",
   "metadata": {},
   "source": [
    "*Note* âš ï¸ Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make the loss a little bit smaller for each batch. Itâ€™s randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice). Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm. Itâ€™s deterministic, meaning you always get the same results when training with the same algorithm on the same corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad221b-603b-4954-9936-b09d47445eba",
   "metadata": {},
   "source": [
    "### Assembling a Corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1697719-7a32-4881-b513-b22b9e092875",
   "metadata": {},
   "source": [
    "Thereâ€™s a very simple API in ğŸ¤— Transformers that you can use to train a new tokenizer with the same characteristics as an existing one: AutoTokenizer.train_new_from_iterator(). To see this in action, letâ€™s say we want to train GPT-2 from scratch, but in a language other than English. Our first task will be to gather lots of data in that language in a training corpus. To provide examples everyone will be able to understand, we wonâ€™t use a language like Russian or Chinese here, but rather a specialized English language: Python code.\n",
    "\n",
    "The ğŸ¤— Datasets library can help us assemble a corpus of Python source code. Weâ€™ll use the usual load_dataset() function to download and cache the CodeSearchNet dataset. This dataset was created for the CodeSearchNet challenge and contains millions of functions from open source libraries on GitHub in several programming languages. Here, we will load the Python part of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4c75ce-c496-458e-b42a-e1c63f1c5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f8eae3-f173-4664-8f72-533124327a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"code_search_net\",\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1e7010-c6a8-44e1-8647-6ed3cd0cd394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 412178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 22176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 23107\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12265553-85f2-415b-ad51-ad2e7d7757ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_all_objects_in_bucket(\n",
      "        aws_bucket_name,\n",
      "        s3_client=None,\n",
      "        max_keys=1000\n",
      "):\n",
      "    \"\"\"\n",
      "    Little utility method that handles pagination and returns\n",
      "    all objects in given bucket.\n",
      "    \"\"\"\n",
      "    logger.debug(\"Retrieving bucket object list\")\n",
      "\n",
      "    if not s3_client:\n",
      "        s3_client, s3_resource = get_s3_client()\n",
      "\n",
      "    obj_dict = {}\n",
      "    paginator = s3_client.get_paginator('list_objects')\n",
      "    page_iterator = paginator.paginate(Bucket=aws_bucket_name)\n",
      "    for page in page_iterator:\n",
      "        key_list = page.get('Contents', [])\n",
      "        logger.debug(\"Loading page with {} keys\".format(len(key_list)))\n",
      "        for obj in key_list:\n",
      "            obj_dict[obj.get('Key')] = obj\n",
      "    return obj_dict\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][650]['whole_func_string'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527a4dd-36ac-492d-8217-733955e7439b",
   "metadata": {},
   "source": [
    "The first thing we need to do is transform the dataset into an iterator of lists of texts â€” for instance, a list of list of texts. Using lists of texts will enable our tokenizer to go faster (training on batches of texts instead of processing individual texts one by one), and it should be an iterator if we want to avoid having everything in memory at once. If your corpus is huge, you will want to take advantage of the fact that ğŸ¤— Datasets does not load everything into RAM but stores the elements of the dataset on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f80152-c72c-450b-9c8f-171e9b5327a4",
   "metadata": {},
   "source": [
    "Using a Python generator, we can avoid Python loading anything into memory until itâ€™s actually necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8fb53f-eeaa-43b3-a37d-ee7f05736b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i: i + 1000]['whole_func_string'] for i in range(0, len(raw_datasets['train']), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e60d88-7852-457a-a50f-010f5955d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = training_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d635a552-6f8e-4fc4-a783-eee39a79f493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def addidsuffix(self, idsuffix, recursive = True):\n",
      "        \"\"\"Appends a suffix to this element's ID, and optionally to all child IDs as well. There is sually no need to call this directly, invoked implicitly by :meth:`copy`\"\"\"\n",
      "        if self.id: self.id += idsuffix\n",
      "        if recursive:\n",
      "            for e in self:\n",
      "                try:\n",
      "                    e.addidsuffix(idsuffix, recursive)\n",
      "                except Exception:\n",
      "                    pass\n"
     ]
    }
   ],
   "source": [
    "res = next(g)\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01bf41-f4e6-4057-8a9a-dc293901ea96",
   "metadata": {},
   "source": [
    "This line of code doesnâ€™t fetch any elements of the dataset; it just creates an object you can use in a Python for loop. The texts will only be loaded when you need them (that is, when youâ€™re at the step of the for loop that requires them), and only 1,000 texts at a time will be loaded. This way you wonâ€™t exhaust all your memory even if you are processing a huge dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea710f4-4a80-4eeb-b657-5b9dabf3d53b",
   "metadata": {},
   "source": [
    "The problem with a generator object is that it can only be used once. So, instead of this giving us the list of the first 10 digits as many times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d1378b6-375f-4b4e-bfa2-dc7f912dde43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gen = (i for i in range(10))\n",
    "print(list(gen))\n",
    "print(list(gen))\n",
    "print(list(gen))\n",
    "print(list(gen))\n",
    "print(list(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f386d94-ef80-4ef2-a24a-36f650c1fd8c",
   "metadata": {},
   "source": [
    "So, To avoid the above problem, we define a function that returns a generator instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecc0b785-ef4f-42e7-80b6-c976561aa6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(): \n",
    "    training_corpus = (\n",
    "    raw_datasets[\"train\"][i: i + 1000]['whole_func_string'] for i in range(0, len(raw_datasets['train']), 1000)\n",
    "    )\n",
    "\n",
    "    return training_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "282480ae-0065-44b4-9a1d-0c0bae3ef4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503ded0-91bf-433a-bd09-9d9b7a38ab56",
   "metadata": {},
   "source": [
    "### Training a New Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9ca5b-eee3-4170-9511-331f86bf2f74",
   "metadata": {},
   "source": [
    "Now, that we have our corpus in form of an iterator of batches of texts, we are more than ready to train a tokenizer. To do this, We will first need to load the tokenizer we want to pair with our model, in this case(GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0929d3-d33a-4dca-84e0-8d4d0036640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4882fb5a-c913-4c7e-832b-c2435fee3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b08e9c-4055-4e17-94c6-d83d27265b07",
   "metadata": {},
   "source": [
    "Even though we are going to train a new tokenizer, It`s a good idea to avoid doing this from scratch. This way, we wont have to specify about the tokenization algoritmn or special tokens we want to use, Our new tokenizer will be exactly the same as gpt2, inspite of the fact that our tokenizer will be trained on a different corpus and will have a new vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810e2f3-fb9a-4561-921b-a880cfe575e9",
   "metadata": {},
   "source": [
    "letâ€™s have a look at how this tokenizer would treat an example function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eee81e6-a42d-4d47-b8c6-5d0f84216c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '.\"', '\"\"', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab4a83-f6d4-43a9-b3a2-a640e7343cbd",
   "metadata": {},
   "source": [
    "This tokenizer has a few special symbols, like Ä  and ÄŠ, which denote spaces and newlines, respectively. As we can see, this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels (since having sets of four or eight spaces is going to be very common in code). It also split the function name a bit weirdly, not being used to seeing words with the _ character.\n",
    "\n",
    "Letâ€™s train a new tokenizer and see if it solves those issues. For this, weâ€™ll use the method `train_new_from_iterator()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbc6c285-71d9-40e0-ae6b-5be4e5a19113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42af01-7cbf-41ea-b5eb-04f469ad5727",
   "metadata": {},
   "source": [
    "*Note* that `AutoTokenizer.train_new_from_iterator()` only works if the tokenizer you are using is a â€œfastâ€ tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b99f463-998b-4af2-9de7-e7cd2541a2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',', 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4ff8b-b6d5-48bf-9efa-7a1eb43c59cf",
   "metadata": {},
   "source": [
    "Here we again see the special symbols Ä  and ÄŠ that denote spaces and newlines, but we can also see that our tokenizer learned some tokens that are highly specific to a corpus of Python functions: for example, there is a ÄŠÄ Ä Ä  token that represents an indentation, and a Ä \"\"\" token that represents the three quotes that start a docstring. The tokenizer also correctly split the function name on _."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db5fdb-7f47-4344-b27b-52ad0c22a6f4",
   "metadata": {},
   "source": [
    "Let`s look at another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d039de2-a817-46c7-9e9a-0af44aa56eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',', 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ba7f067-8878-477b-bd05-3de5f7e5eecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1422, 1271, 1110, 156, 7777, 2497, 6871, 1105, 146, 1250, 1120, 20164, 10932, 10289, 1107, 1803, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = tokenizer(example)\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72dda4-fa88-4512-820c-b99250eb613f",
   "metadata": {},
   "source": [
    "In addition to the token corresponding to an indentation, here we can also see a token for a double indentation: ÄŠÄ Ä Ä Ä Ä Ä Ä . The special Python words like class, init, call, self, and return are each tokenized as one token, and we can see that as well as splitting on _ and . the tokenizer correctly splits even camel-cased names: LinearLayer is tokenized as [\"Ä Linear\", \"Layer\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6c68d-4860-4b41-8868-dd097f6a1b46",
   "metadata": {},
   "source": [
    "## Saving the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5871ea32-94b6-4bf6-856e-3b754502067e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer/code-search-net-tokenizer/tokenizer_config.json',\n",
       " './tokenizer/code-search-net-tokenizer/special_tokens_map.json',\n",
       " './tokenizer/code-search-net-tokenizer/vocab.json',\n",
       " './tokenizer/code-search-net-tokenizer/merges.txt',\n",
       " './tokenizer/code-search-net-tokenizer/added_tokens.json',\n",
       " './tokenizer/code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./tokenizer/code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af66fc-ea56-4a60-8410-ecf66c7d8dbe",
   "metadata": {},
   "source": [
    "## Fast Tokenizers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cadbab4-48b4-4189-8fe3-d354418cc51f",
   "metadata": {},
   "source": [
    "*Note* When tokenizing a single sentence, you wonâ€™t always see a difference in speed between the slow and fast versions of the same tokenizer. In fact, the fast version might actually be slower! Itâ€™s only when tokenizing lots of texts in parallel at the same time that you will be able to clearly see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910028c-1491-4f55-b359-cd3d32a56394",
   "metadata": {},
   "source": [
    "## Batch Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f1814-3edb-4348-b993-b1f813e878b8",
   "metadata": {},
   "source": [
    "Besides the parallelization capabilities of fast tokenizers, the key functionality of fast tokenizers is that they always keep track of the original span of texts the final tokens come from â€” a feature we call offset mapping. This in turn unlocks features like mapping each word to the tokens it generated or mapping each character of the original text to the token itâ€™s inside, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b2eb03-93bb-458a-96e6-2f09f60791c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be5b767f-8024-4138-bb66-8b292bff66eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'> {'input_ids': [101, 1422, 1271, 1110, 156, 7777, 2497, 6871, 1105, 146, 1250, 1120, 20164, 10932, 10289, 1107, 1803, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "example = \"My name is Sylvavin and I work at Hugging Face in Canada\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding), encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2926dde-d3ea-405b-a96c-b266a74652f2",
   "metadata": {},
   "source": [
    "Since the AutoTokenizer class picks a fast tokenizer by default, we can use the additional methods this BatchEncoding object provides. We have two ways to check if our tokenizer is a fast or a slow one. We can either check the attribute is_fast of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8aabeaa8-6576-4534-85c9-0904e34caf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df868b90-89e6-46d7-9f7e-1659712e3eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or check the same attribute of our encoding\n",
    "encoding.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb3cda-0f68-4734-9c22-db19332a58d6",
   "metadata": {},
   "source": [
    "Letâ€™s see what a fast tokenizer enables us to do. First, we can access the tokens without having to convert the IDs back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84011d4f-70e1-4da7-bcd8-59aa5550e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##vin', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Canada', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1141d-ad49-404a-9e1b-daefb8b03db1",
   "metadata": {},
   "source": [
    "In this case the token at index 5 is ##yl, which is part of the word â€œSylvainâ€ in the original sentence. We can also use the word_ids() method to get the index of the word each token comes from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e038bef4-1c01-423d-b130-75b722651978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, None]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70b3a8-045e-4c02-b0b2-2d13197c5f18",
   "metadata": {},
   "source": [
    "- We can see that the tokenizerâ€™s special tokens [CLS] and [SEP] are mapped to None\n",
    "- Each token is mapped to the word it originates from. This is especially useful to determine if a token is at the start of a word or if two tokens are in the same word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ffdf5-5f01-4b91-b113-b193bfe5577a",
   "metadata": {},
   "source": [
    "Lastly, we can map any word or token to characters in the original text, and vice versa, via the word_to_chars() or token_to_chars() and char_to_word() or char_to_token() methods. For instance, the word_ids() method told us that ##yl is part of the word at index 3, but which word is it in the sentence? We can find out like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c64f0d5-164e-448c-8277-da3d2f48e72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(6)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac27fd-2432-45ee-93ac-73e2c44f1ceb",
   "metadata": {},
   "source": [
    "this is all powered by the fact the fast tokenizer keeps track of the span of text each token comes from in a list of offsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472ca8c-47c5-48aa-bf06-29002e20d454",
   "metadata": {},
   "source": [
    "## Inside the token-classification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa2ba81-9579-41b0-86b6-de9df8a5ce36",
   "metadata": {},
   "source": [
    "### Getting the base results with the pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a03c0-497a-4aef-aca9-d1b45116a53a",
   "metadata": {},
   "source": [
    "First, letâ€™s grab a token classification pipeline so we can get some results to compare manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5305cc1-e390-4828-91e5-24de30e0fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef76a5f1-dbdb-4193-ae30-0682f6164ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "token_classifier = pipeline('token-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e852e7de-2ab6-43fc-ad9c-82d6af829051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99938285,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99815494,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9959072,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99923277,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.97611505,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887976,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9932106,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb870c21-92de-46ee-9979-770f921d8e10",
   "metadata": {},
   "source": [
    "We can also ask the pipeline to group together the tokens that correspond to the same entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d50e6540-d946-419e-a846-cbc0ef087758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier = pipeline('token-classification', aggregation_strategy='simple')\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6c9fb-69d9-41bd-a7e8-64d56464ed7b",
   "metadata": {},
   "source": [
    "### From Inputs to Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f60d3e89-1489-481a-911a-a970cb327925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,TFAutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81db98b3-0370-4596-8548-9e3496b8adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 15:51:31.776131: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-21 15:51:31.776801: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-21 15:51:31.776805: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-21 15:51:31.776903: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-21 15:51:31.777713: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'dbmdz/bert-large-cased-finetuned-conll03-english'\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb05fd20-afd5-4b8f-94a6-40ed44614d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "031bbd37-a8cf-4fd1-910e-c5f23293d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "182bc7ff-9cf3-4136-aee6-76fedf3e23be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  101  1422  1271  1110   156  7777  2497  1394  1105   146  1250  1120\n",
      "  20164 10932 10289  1107  6010   119   102]], shape=(1, 19), dtype=int32)\n",
      "(1, 19, 9)\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'])\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba9487-415b-4e94-9db2-331d2c6eb7e1",
   "metadata": {},
   "source": [
    "So, As we know for the text classification pipeline, we use a softmax function to convert those logits to probabilities, and we take the argmax to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47552ae9-0a8d-4b72-bab5-f5ad5b2a4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a89b95e7-9c0c-43bd-8ab5-6981e8556e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]\n",
    "probabilities = probabilities.numpy().tolist()\n",
    "predictions = tf.math.argmax(outputs.logits, axis=-1)[0]\n",
    "predictions = predictions.numpy().tolist()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabdebc-b1bf-4857-8419-1e1f96015de8",
   "metadata": {},
   "source": [
    "The model.config.id2label attribute contains the mapping of indexes to labels that we can use to make sense of the predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f45082b6-b411-4cae-a95d-7d4f1186d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8f045d2-8528-43ba-a69c-efc30d46e013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'},\n",
       " {'entity': 'I-PER', 'score': 0.9981548190116882, 'word': '##yl'},\n",
       " {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'},\n",
       " {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'},\n",
       " {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'},\n",
       " {'entity': 'I-ORG', 'score': 0.9761150479316711, 'word': '##gging'},\n",
       " {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face'},\n",
       " {'entity': 'I-LOC', 'score': 0.99321049451828, 'word': 'Brooklyn'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions): \n",
    "    label = model.config.id2label[pred]\n",
    "\n",
    "    if label != 'O': \n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred],\"word\": tokens[idx]}\n",
    "        )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc6c9e-8e55-4fac-907f-9fdbc53d4f1f",
   "metadata": {},
   "source": [
    "This is very similar to what we had before, with one exception: the pipeline also gave us information about the start and end of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set return_offsets_mapping=True when we apply the tokenizer to our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb4226ae-801b-428b-99e7-552368a50821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 14),\n",
       " (14, 16),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 32),\n",
       " (33, 35),\n",
       " (35, 40),\n",
       " (41, 45),\n",
       " (46, 48),\n",
       " (49, 57),\n",
       " (57, 58),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3713c9b-8d32-4e10-bf03-2b1b3f0f6a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yl'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[12:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561dfd2-0b31-427a-b856-09f1859f35a6",
   "metadata": {},
   "source": [
    "See, We get the proper span of text without ##, Using this we can now complete the previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d752fe89-192e-48be-892b-597a71bccdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.9993828535079956,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9981548190116882,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.995907187461853,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9992327690124512,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931059837341,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9761150479316711,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887974858283997,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99321049451828,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975f779-b722-4fef-9c02-02778100677f",
   "metadata": {},
   "source": [
    "The results are same as what we got from the first pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a365afdd-68dc-4a47-a909-4d9b48b333b5",
   "metadata": {},
   "source": [
    "### Grouping Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19f21998-9827-48f9-94ff-369f11045033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d83cdac3-1e90-4ef9-b77e-0ebec9f01884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.998169407248497,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796018799146017,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99321049451828,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [] \n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets['offset_mapping']\n",
    "\n",
    "idx = 0 \n",
    "\n",
    "while idx < len(predictions): \n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "\n",
    "    if label != 'O': \n",
    "        label = label[2:] # remove B- or I-\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # grab all tokens labelled with I-Label\n",
    "        all_scores = []\n",
    "\n",
    "        while (idx < len(predictions) and model.config.id2label[predictions[idx]] == f\"I-{label}\"):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "\n",
    "        results.append({\n",
    "            \"entity_group\": label, \n",
    "            \"score\": score, \n",
    "            \"word\": word, \n",
    "            \"start\": start, \n",
    "            \"end\": end\n",
    "        })\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ce183-4b2c-4dd9-8bef-ae3f726e52d4",
   "metadata": {},
   "source": [
    "Woah! And here we have it gentlemen, same results as the second pipeline "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
