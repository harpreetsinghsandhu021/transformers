{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b400cec-0806-480f-834b-ad39182159fd",
   "metadata": {},
   "source": [
    "## Behind the Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58aa4f48-7d05-414c-8f48-af7702ae5a35",
   "metadata": {},
   "source": [
    "Let‚Äôs start with a complete example, taking a look at what happened behind the scenes when we executed the following code in the `pipelines` notebook.\n",
    "\n",
    "<img src='./images/arc.png' width='800' height='200' style='border-radius:10px; margin-left:auto; margin-right:auto' />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e053d-c756-424e-bec0-3d6cf72c82ee",
   "metadata": {},
   "source": [
    "## Preprocessing with a tokenizer  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f009d-fea2-4bbb-943c-d93e99aa12ad",
   "metadata": {},
   "source": [
    "Like other neural networks, Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:\n",
    "\n",
    "Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "Mapping each token to an integer\n",
    "Adding additional inputs that may be useful to the model\n",
    "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the Model Hub. To do this, we use the AutoTokenizer class and its from_pretrained() method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f7acde-bc34-4d14-84d0-76bf229185cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64f7038-4161-48eb-aa9c-6e51f1ce4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1ce365-97ba-4f98-b06f-8fec8bffdd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd21be-80b6-4d58-a59c-f9c35592305f",
   "metadata": {},
   "source": [
    "Once we have the tokenizer, we can directly pass our sentences to it and we‚Äôll get back a dictionary that‚Äôs ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "261603ba-2f8d-4468-8d68-9ff528fceb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"this is the best thing happened to me\", \n",
    "    \"I hate this so much!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5229a421-bd3e-4ebe-a698-f3749578daab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
       "array([[ 101, 2023, 2003, 1996, 2190, 2518, 3047, 2000, 2033,  102],\n",
       "       [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102,    0,    0]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='tf')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f243f21-e632-4a95-b9b9-656d5edce942",
   "metadata": {},
   "source": [
    "The output itself is a dictionary containing two keys, `input_ids` and `attention_mask`. `input_ids` contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. \n",
    "\n",
    "We can download our pretrained model the same way we did with our tokenizer. ü§ó Transformers provides an TFAutoModel class which also has a from_pretrained method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fa80fbf-f878-429d-803d-018cd53d3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel \n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b9b20c6-b385-443a-8e04-66dae6513f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ca08f-859d-4d75-a328-8b1b5bd8ba2f",
   "metadata": {},
   "source": [
    "This architecture contains only the base transformer module, given some inputs, it outputs what we'll call `hidden states`, also known as features. For each model Input, we'll retrieve a high dimensional vector representing the contextual understanding of that input by the transformer model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515b303-4f89-40ca-a285-f6cb294a56b1",
   "metadata": {},
   "source": [
    "While these hidden states can be useful on their own, they‚Äôre usually inputs to another part of the model, known as the head. In pipelines notebook, the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it.\n",
    "\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "- Batch size: The number of sequences processed at a time (2 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "- Hidden size: The vector dimension of each model input.\n",
    "\n",
    "It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
    "\n",
    "We can see this if we feed the inputs we preprocessed to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f44d0348-1996-415a-93de-df3c3b43dcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 768)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa750b0-2e41-49a0-98dd-39495fc43eef",
   "metadata": {},
   "source": [
    "*Note* that the outputs of the huggingface transformer models behave like namedtuples or dictionaries. We can access the elements by attributes or by key or even by index. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bfbfa-03a4-4682-84bc-276cad3c9a0e",
   "metadata": {},
   "source": [
    "## Model Heads\n",
    "The Model heads take the high dimensionsal vector of hidden states as input and project onto a different dimension: \n",
    "\n",
    "\n",
    "<img src='./images/arc2.png' width='800' height='200' style='border-radius:10px; margin-left:auto; margin-right:auto' />\n",
    "\n",
    "The output of the Transformer model is sent directly to the model head to be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd404b2-a2ed-475a-806d-6a60f4d7de56",
   "metadata": {},
   "source": [
    "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won‚Äôt actually use the `TFAutoModel` class, but `TFAutoModelForSequenceClassification`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4219e271-f914-4739-a7c2-39eb08dac32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18c68af6-9c9c-402d-b017-56494b2dd06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62b95bde-4967-4bb1-a53c-1cacda820135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30786663-52bb-48de-aa09-5debc2a5b6f8",
   "metadata": {},
   "source": [
    "*Note* the shape of our outputs, the dimensionality is much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label)\n",
    "\n",
    "Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb642018-0f11-4b3e-980b-0d9374684547",
   "metadata": {},
   "source": [
    "## Post-Processing the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91642a29-9f0a-4e16-a9d1-48553655bbf5",
   "metadata": {},
   "source": [
    "The values we get as output from our model don‚Äôt necessarily make sense by themselves. Let‚Äôs take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c09510fc-e02d-49fb-a849-f0874ae3a5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.70115715,  1.0346943 ],\n",
       "       [ 4.169231  , -3.3464472 ]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7dde4-34f5-4c65-939d-2481610604fa",
   "metadata": {},
   "source": [
    "Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer (all ü§ó Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e7a0a60-504c-42f0-92ef-f8f8aae90b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.4984064e-01 8.5015941e-01]\n",
      " [9.9945587e-01 5.4418418e-04]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687443f5-3db5-49aa-9e97-0e44703929fe",
   "metadata": {},
   "source": [
    "To get the labels corresponding to each position, we can inspect the id2label attribute of the model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b23de70c-ce82-48f7-aa14-ce2bdcd062c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99051901-c736-4a84-8059-e38d1c5f069e",
   "metadata": {},
   "source": [
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "First prediction [1.4984064e-01 8.5015941e-01]:\n",
    "- NEGATIVE (0): 0.1498 or ~15% confidence\n",
    "- POSITIVE (1): 0.8502 or ~85% confidence\n",
    "Conclusion: The model strongly predicts this is a POSITIVE sentiment (85% confidence)\n",
    "\n",
    "Second prediction [9.9945587e-01 5.4418418e-04]:\n",
    "- NEGATIVE (0): 0.9995 or ~99.95% confidence\n",
    "- POSITIVE (1): 0.0005 or ~0.05% confidence\n",
    "Conclusion: The model is extremely confident this is a NEGATIVE sentiment (99.95% confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833c3f1-ce5e-46de-a475-1a7efcc03ed9",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955981b-a0b0-40d7-b564-4f39ca2f7161",
   "metadata": {},
   "source": [
    "In this section we‚Äôll take a closer look at creating and using a model. We‚Äôll use the TFAutoModel class, which is handy when you want to instantiate any model from a checkpoint.\n",
    "\n",
    "The TFAutoModel class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It‚Äôs a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n",
    "\n",
    "However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Let‚Äôs take a look at how this works with a BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c7e22-cbf3-4b43-868b-9b9a1e044ced",
   "metadata": {},
   "source": [
    "## Creating a Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a0c8584-c941-49c4-8558-99d9f006c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel\n",
    "\n",
    "config = BertConfig()\n",
    "\n",
    "model = TFBertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bf0ee21-8eb8-442e-8ec1-141e16924a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da5a5d2-d407-42c3-b33c-df17b9aab837",
   "metadata": {},
   "source": [
    "Creating a model from the default configuration initializes it with random values. Loading a Transformer model that is already trained is simple ‚Äî we can do this using the `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "667c726f-180c-47ad-89c0-644d69c68ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74797310-cfa4-4c25-a7ad-53e9e761162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model \n",
    "model.save_pretrained('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31201316-027b-430c-8293-f46e50e012ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8be8eebb-4b3d-4974-b86f-552d488e5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sequences, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6118a3b6-5156-4a70-b5ca-871443831077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[ 101, 7592,  999,  102],\n",
       "       [ 101, 4658, 1012,  102],\n",
       "       [ 101, 3835,  999,  102]], dtype=int32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15d17036-5eb6-4a19-9799-577e5aec4ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tf.constant(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad5942ef-22bf-418e-9351-dcd0650aed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91762260-7ab8-4556-9aea-3228bcfdf9d2",
   "metadata": {},
   "source": [
    "## Handling Multiple Sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a5e79-c52e-4754-9e65-8a236f291b6a",
   "metadata": {},
   "source": [
    "### Models expect a batch of Inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "717bc7ca-ca36-49e8-9ce5-6caa71d9b02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7aae829-cc47-4e7e-9a8d-e621863cded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "109da16d-4708-406c-94ab-8be86ca782f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tf.constant(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b52e46e2-f99c-471b-bab5-d15c7e987439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.7276208,  2.8789375]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf426540-9f0f-46c6-a6a7-f259a093a8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
      "   2878  2166  1012   102]], shape=(1, 16), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors='tf')\n",
    "print(tokenized_inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06d72613-df1e-48e3-b355-092bf31ab2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits tf.Tensor([[-2.7276208  2.8789375]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output = model(input_ids)\n",
    "print(\"logits\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04a4ff-7872-42b7-881f-8ecc4880acaf",
   "metadata": {},
   "source": [
    "Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b8164c8-14b1-4c80-bd3a-d46840cc107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [ids, ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd046adc-ce85-464a-949d-5cfad9257619",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.constant(batched_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6514120-3be7-4377-ba66-5453d75c6b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-2.7276218,  2.8789384],\n",
       "       [-2.7276218,  2.8789384]], dtype=float32)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_ids)\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d307b-d83f-4a96-b1a1-478b8412e292",
   "metadata": {},
   "source": [
    "Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. There‚Äôs a second issue, though. When you‚Äôre trying to batch together two (or more) sentences, they might be of different lengths. If you‚Äôve ever worked with tensors before, you know that they need to be of rectangular shape, so you won‚Äôt be able to convert the list of input IDs into a tensor directly. To work around this problem, we usually pad the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7e985-5a4e-45f1-be80-30e6eea4bf6d",
   "metadata": {},
   "source": [
    "### Padding the Inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101f12f-89ee-4719-9004-2ca87163168f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following list of lists cannot be converted to a tensor\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]\n",
    "tf.constant(batched_ids) # This returns ValueError: Can't convert non-rectangular Python sequence to Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d2896-123b-403a-b545-f5c95da39566",
   "metadata": {},
   "source": [
    "In order to work around this, we‚Äôll use padding to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values. For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. In our example, the resulting tensor looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2a4946c-137d-4c31-93fe-7ce5a6efd52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[200, 200, 200],\n",
       "       [200, 200, 100]], dtype=int32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]\n",
    "\n",
    "tf.constant(batched_ids) # Converted to tensorflow tensor successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a70c7-b564-4964-84d8-e0f260f0cc71",
   "metadata": {},
   "source": [
    "The padding token ID can be found in tokenizer.pad_token_id. Let‚Äôs use it and send our two sentences through the model individually and batched together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f96d1fff-74fa-4385-80a8-fb7bdf18b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cb4cbf2-5750-4d07-8b7d-2150109c7135",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]] \n",
    "batched_ids = [\n",
    "    [200, 200, 200], \n",
    "    [200, 200, tokenizer.pad_token_id]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "634a0b0a-13ae-4c56-bdc4-735a4df51638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 1.5693657 -1.3894563]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[ 0.5802985  -0.41252238]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.5693657 -1.3894563]\n",
      " [ 1.3373483 -1.2163186]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(model(tf.constant(sequence1_ids)).logits)\n",
    "print(model(tf.constant(sequence2_ids)).logits)\n",
    "print(model(tf.constant(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531632a-18a8-46a7-a572-d41038ce9d5b",
   "metadata": {},
   "source": [
    "There‚Äôs something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we‚Äôve got completely different values!\n",
    "\n",
    "This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce3ff8-0818-45dd-b831-53d284be62bd",
   "metadata": {},
   "source": [
    "### Attention Masks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69bbc46-5e1a-434a-8c3d-6411130fbca5",
   "metadata": {},
   "source": [
    "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
    "\n",
    "Let‚Äôs complete the previous example with an attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7dbb4a7d-ba1d-4409-8b1a-22bb66b829d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1,1,1], \n",
    "    [1,1,0]\n",
    "]\n",
    "\n",
    "outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2cc68c8f-99a3-4126-9d9b-4df357f049ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 1.5693657, -1.3894563],\n",
       "       [ 0.5802984, -0.4125224]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff43b5-361d-4ab5-a924-a0917071a59d",
   "metadata": {},
   "source": [
    "Now we get the same logits for the second sentence in the batch.\n",
    "\n",
    "Notice how the last value of the second sequence is a padding ID, which is a 0 value in the attention mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b45bed-c785-4ac0-ba44-e32dcff5a4da",
   "metadata": {},
   "source": [
    "## Special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca74ab-48ef-4e24-bd1b-b7e82340b631",
   "metadata": {},
   "source": [
    "If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "119d2b9a-810a-436e-8c82-da5574eb2188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491304f-7419-43a2-a6f1-35cf68d204f7",
   "metadata": {},
   "source": [
    "One token ID was added at the beginning, and one at the end. Let‚Äôs decode the two sequences of IDs above to see what this is about: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e0e17c03-94f7-4c2b-a0ec-198fd81f99b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89902c73-76ac-4352-80d8-977553861a1a",
   "metadata": {},
   "source": [
    "The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
